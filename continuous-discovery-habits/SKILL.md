---
name: continuous-discovery-habits
description: 'Product discovery framework based on Teresa Torres'' "Continuous Discovery Habits". Use when you need to: (1) build an opportunity solution tree from desired outcomes, (2) identify and prioritize customer opportunities, (3) design assumption tests for product ideas, (4) structure customer interview snapshots, (5) map assumptions to experiments, (6) move from output-driven to outcome-driven product development.'
license: MIT
metadata:
  author: Tomasz Staniak
  version: "1.0.0"
---

# Continuous Discovery Habits

A structured, repeatable approach to product discovery that helps teams make better product decisions through weekly customer touchpoints, opportunity mapping, and rapid assumption testing. Based on Teresa Torres' "Continuous Discovery Habits."

## Core Principle

**Good product teams don't debate opinions — they test assumptions.** The goal of discovery is not to validate ideas, but to discover opportunities and find the best path to a desired outcome. Discovery is not a phase; it is a continuous, weekly habit embedded into how teams work.

The foundation: most product teams skip discovery entirely, jumping from a business outcome directly to a solution. The Opportunity Solution Tree (OST) provides the missing structure — it forces teams to explore the problem space before committing to solutions, and to compare multiple solutions against clear criteria before building anything.

## Scoring

**Goal: 10/10.** When reviewing product discovery work, rate it 0-10:

| Score | Description |
|-------|-------------|
| 0-2 | No discovery. Team builds what stakeholders request or what feels right. No customer contact. |
| 3-4 | Sporadic discovery. Occasional user research, but not connected to decisions. Ideas come from inside the building. |
| 5-6 | Some structure. Team talks to customers but lacks a systematic way to prioritize opportunities or test assumptions. |
| 7-8 | Strong discovery. OST exists, weekly customer touchpoints, assumption testing before building. |
| 9-10 | Exceptional. Full OST discipline, compare-and-contrast experiments, story-based interviews, and the whole team participates in discovery weekly. |

## The Opportunity Solution Tree (OST)

The core visual framework connecting business outcomes to solutions through opportunities:

```
        Desired Outcome
        (metric to move)
              │
    ┌─────────┼─────────┐
    ▼         ▼         ▼
Opportunity Opportunity Opportunity
 (unmet need) (pain point) (desire)
    │         │
  ┌─┴─┐    ┌─┴─┐
  ▼   ▼    ▼   ▼
Sol. Sol.  Sol. Sol.
  │         │
  ▼         ▼
Assumption Assumption
  Tests      Tests
```

**Rules for a good OST:**
- **One desired outcome** at the top — the metric the team is trying to move
- **Opportunities** are customer needs, pain points, or desires — never solutions
- **Solutions** address specific opportunities — each solution maps to at least one opportunity
- **Assumption tests** validate the riskiest assumptions before building
- The tree is a living document, updated weekly

### 1. Setting the Desired Outcome

**Core concept:** A product trio (PM, designer, engineer) needs a single, clear outcome to optimize for. This outcome should be a measurable business or product metric that the team can directly influence.

**Why it works:** Without a clear outcome, teams optimize for output (ship features) instead of impact. A well-chosen outcome gives the team autonomy to find the best path while staying aligned with business goals.

**Key insights:**
- Outcomes should be lagging indicators of customer value (retention, activation, task success rate), not business vanity metrics (revenue alone)
- Teams need the authority to choose their own path to the outcome
- "Increase revenue" is too broad; "Increase 30-day retention for new users from 40% to 55%" is actionable
- Product outcomes (what customers do) lead to business outcomes (what the company gets)

**Product applications:**

| Context | Application | Example |
|---------|-------------|---------|
| Quarterly planning | Assign outcomes, not features, to teams | "Improve onboarding completion rate" instead of "Build new onboarding flow" |
| OKR setting | Use product outcomes as key results | KR: "Increase 7-day activation from 30% to 45%" |
| Roadmap review | Check if planned work connects to outcomes | If a feature doesn't map to an outcome, question why it's being built |

**Ethical boundary:** Never manipulate outcomes to justify pre-decided solutions. The outcome must come before the solution, not be reverse-engineered to support it.

See: [references/desired-outcomes.md](references/desired-outcomes.md)

### 2. Discovering Opportunities

**Core concept:** Opportunities are unmet customer needs, pain points, and desires discovered through continuous customer interviews. They live in the middle layer of the OST, between the outcome and solutions.

**Why it works:** Most teams jump from outcome to solution. The opportunity layer forces teams to understand the problem space first. Multiple opportunities exist for any outcome; choosing the right one to address is a strategic decision.

**Key insights:**
- Opportunities come from customer stories, not from asking "what do you want?"
- Use story-based interviews: "Tell me about the last time you [did the thing]"
- Extract opportunities from stories — pain points, workarounds, unmet needs
- Organize opportunities in a hierarchy (parent opportunities break into sub-opportunities)
- Prioritize by frequency, intensity, and alignment with the desired outcome

**The Interview Snapshot:**

| Field | Content |
|-------|---------|
| Quick facts | Date, participant, context |
| Key moments | 3-5 notable quotes or behaviors from the interview |
| Opportunities | Unmet needs or pain points identified |
| Insights | Patterns or surprises worth noting |

**Product applications:**

| Context | Application | Example |
|---------|-------------|---------|
| Weekly interviews | Extract opportunities from each interview | "Users struggle to find where they left off after a break" |
| Opportunity prioritization | Score by frequency + severity + outcome alignment | High frequency + high severity + directly tied to retention = top priority |
| Stakeholder alignment | Share the opportunity tree, not just ideas | "We found 3 major opportunities. Here's why we're addressing this one first." |

**Copy patterns:**
- Frame features around the opportunity they solve: "Never lose your place — pick up exactly where you left off"
- Use customer language from interviews, not internal jargon

**Ethical boundary:** Never cherry-pick interview quotes to support a preferred solution. Report all opportunities honestly, even if they challenge existing plans.

See: [references/opportunity-discovery.md](references/opportunity-discovery.md)

### 3. Generating and Evaluating Solutions

**Core concept:** For each target opportunity, generate multiple solutions (at least 3) and compare them. Never go with the first idea. Use compare-and-contrast thinking to find creative combinations.

**Why it works:** The first idea is rarely the best. By forcing multiple options, teams escape anchoring bias and find solutions that are simpler, cheaper, or more effective than the obvious choice.

**Key insights:**
- Generate at least 3 solutions per opportunity
- Mix solution types: product changes, process changes, service changes, content changes
- Use "How Might We" (HMW) framing to generate ideas: "How might we help users pick up where they left off?"
- Evaluate solutions against effort, impact, and assumption risk
- Combine elements from different solutions — the best solution is often a hybrid

**Product applications:**

| Context | Application | Example |
|---------|-------------|---------|
| Ideation | Generate 3+ solutions for top opportunity | For "users lose their place": (1) auto-save with resume, (2) activity timeline, (3) daily digest email |
| Evaluation | Compare on effort/impact/risk | Auto-save: low effort, high impact, low risk → start here |
| Design review | Check that solutions map to opportunities | "Which opportunity does this feature address? Is it the top-priority one?" |

**Copy patterns:**
- "We explored [N] approaches before choosing the one that best addresses [opportunity]"
- "Instead of building the obvious solution, we tested [alternatives] and found [insight]"
- "This solution combines the best of [approach A] and [approach B]"

**Ethical boundary:** Never pre-select a solution and reverse-engineer the comparison to justify it. Compare-and-contrast must be genuine — if the process always picks the first idea, it's theater.

See: [references/solution-generation.md](references/solution-generation.md)

### 4. Assumption Mapping and Testing

**Core concept:** Every solution is built on assumptions. Before building, identify the riskiest assumptions and test them with the smallest possible experiment. Map assumptions across four categories: desirability, viability, feasibility, and usability.

**Why it works:** Building is the most expensive way to test an idea. Most solutions fail because of one or two wrong assumptions. Finding those early saves weeks or months of wasted work.

**Assumption categories:**

| Category | Question | Example test |
|----------|----------|-------------|
| Desirability | Will customers want this? | Show a prototype, measure interest |
| Viability | Will this work for the business? | Model unit economics |
| Feasibility | Can we build this? | Technical spike or proof of concept |
| Usability | Can customers use this? | Unmoderated usability test |

**The Assumption Testing Ladder:**

| Level | Method | Speed | Confidence |
|-------|--------|-------|------------|
| 1 | One-question survey | Hours | Low |
| 2 | Fake door / smoke test | Days | Medium |
| 3 | Prototype test (unmoderated) | Days | Medium-High |
| 4 | Concierge / Wizard of Oz | Weeks | High |
| 5 | Live product experiment (A/B) | Weeks | Highest |

**Key insights:**
- Test the riskiest assumption first — the one that, if wrong, kills the whole idea
- Use the smallest experiment that will give you confidence
- "Will customers use it?" is almost always riskier than "Can we build it?"
- Set success criteria before running the test — don't rationalize results after

**Product applications:**

| Context | Application | Example |
|---------|-------------|---------|
| Before sprint planning | List top 3 assumptions for any committed work | "We assume users will notice the new button" → test with a 5-second test |
| After ideation | Pick the riskiest assumption from the winning solution | Desirability risk: "Do users actually care about resuming?" → survey + prototype |
| Retrospective | Review which assumptions were validated vs. invalidated | Track learning velocity: assumptions tested per week |

**Copy patterns:**
- "Before building, we tested our riskiest assumption: [assumption]. Here's what we found."
- "We set a success threshold of [X] before running the experiment — the result was [Y]"
- "This experiment disproved our assumption that [belief], saving us [time/money] in wasted development"

**Ethical boundary:** Design experiments to learn, not to confirm. If you're only running tests that validate your idea, you're doing it wrong.

See: [references/assumption-testing.md](references/assumption-testing.md)

## Common Mistakes

| Mistake | Why It Fails | Fix |
|---------|-------------|-----|
| Skipping opportunities, jumping to solutions | You might build the right solution to the wrong problem | Always fill the opportunity layer before ideating solutions |
| Interviewing only when "doing research" | Discovery becomes a phase, not a habit | Schedule 1+ customer interview per week, every week |
| Asking customers what they want | Customers describe solutions, not jobs or needs | Ask about past behavior: "Tell me about the last time..." |
| Testing only desirability | Ignores feasibility, viability, and usability risks | Map assumptions across all four categories |
| One solution per opportunity | First idea bias; no basis for comparison | Generate at least 3 solutions, then compare |
| No success criteria before testing | You'll rationalize any result as positive | Define "we'll proceed if X" before running the test |

## Quick Diagnostic

| Question | If No | Action |
|----------|-------|--------|
| Do you have a clear desired outcome (metric)? | Team is output-driven | Define a product outcome with the leadership team |
| Did you talk to a customer this week? | Discovery is a phase, not a habit | Schedule a weekly recurring interview slot |
| Do you have an opportunity solution tree? | No structure connecting outcomes to solutions | Build one in 30 minutes with the product trio |
| Did you consider 3+ solutions? | First-idea bias | Run a 15-minute HMW brainstorm |
| Did you test your riskiest assumption before building? | Building is your test (expensive) | Pick one assumption and design a 1-day experiment |

## Reference Files

- [Desired Outcomes Guide](references/desired-outcomes.md) — How to set product outcomes vs. business outcomes, outcome anti-patterns, and examples of well-written outcomes across different product stages
- [Opportunity Discovery](references/opportunity-discovery.md) — Story-based interview techniques, opportunity extraction, interview snapshot templates, and opportunity hierarchy construction
- [Solution Generation](references/solution-generation.md) — How Might We techniques, compare-and-contrast evaluation, solution combination strategies, and avoiding first-idea bias
- [Assumption Testing](references/assumption-testing.md) — Assumption mapping across desirability/viability/feasibility/usability, experiment design templates, the testing ladder in detail, and success criteria frameworks

## Further Reading

- [Continuous Discovery Habits by Teresa Torres](https://www.amazon.com/Continuous-Discovery-Habits-Discover-Products/dp/1736633309?tag=wondelai00-20)
- [Opportunity Solution Tree template (producttalk.org)](https://www.producttalk.org)

## About the Author

Teresa Torres is a product discovery coach who has worked with hundreds of product teams at companies like Netflix, Microsoft, and Spotify. She created the Opportunity Solution Tree framework and advocates for weekly customer touchpoints as a core product team habit.
